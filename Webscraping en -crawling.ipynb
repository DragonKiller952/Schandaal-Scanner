{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper en -crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aan de hand van het vorige onderzoek, waarbij gekeken werd naar de beste methodes om een gegeven website te scrapen, werdt er aangegeven dat de vervolgstap zou zijn om het systeem te koppelen met een webcrawler om zo te kunnen zoeken naar websites aan de hand van een gegeven query. In dit document wordt er gekeken naar het maken van de crawler, waarbij gekeken wordt naar de beste methodes en hoe deze toegepast kunnen worden.\n",
    "\n",
    "## Scraper\n",
    "\n",
    "Zoals in het vorige onderzoek is aangegeven is er uiteindelijk gekozen voor een combinatie van Beautiful Soup en mechanize om data van webpagina's te scrapen. In dit onderzoek is dezelfde code hiervoor gebruikt, met een paar aanpassingen om het goed samen te laten werken met de rest van de systemen die ontwikkeld zijn.\n",
    "\n",
    "## Search engine\n",
    "\n",
    "Om te kunnen webcrawlen moet er een manier zijn om te kunnen zoeken naar websites om te scrapen. Een Search engine zoals google gebruiken is hiervoor een goede oplossing, aangezien dit de mogelijkheid geeft om aan de hand van een enkele zoek-query een lijst aan mogelijk nuttige links kan geven om te scrapen. Een probleem hiermee is echter dat er in de gebruiksvoorwaarden van google is aangegeven dat het niet is toegestaan om google te scrapen, **en het wordt daarom ook heel moeilijk gemaakt door Google om dit wel te doen**. **Een manier om toch gebruik te kunnen maken van de Google search engine is door gebruik te maken van de Google JSON Search API.**\n",
    "\n",
    "### API\n",
    "\n",
    "De Google JSON Search API is een API gecreeërd door Google die gebruikers de mogelijkheid geeft om data op te halen uit de google search engine, zonder dat de daatwerkelijke search engine gescrapet hoeft te worden. Dit geeft een gemakkelijke en legale manier om links op te kunnen halen voor webscraping. Als gekeken wordt naar de top 10 resultaten wanneer zelf wordt gezocht tegenover de Google API, produceert het resultaten die 20% verschillen[1], maar aangezien het scrapen van data op search engines beperkt is en niet toegestaan is binnen hun gebruiksovereenkomst is het gebruiken van API's de optimale manier om data te verzamelen.[2]\n",
    "\n",
    "## Crawler\n",
    "\n",
    "De webcrawler, ook wel Spider genoemt, is een uiteindelijke combinatie van de search engine en de scraper. Hierbij haalt de crawler aan de hand van de gegeven query 50 links op, waarna hij deze een voor een scrapet en de uiteindelijke data returnt in een lijst. Bij het scrapen wordt er deze keer gebruik gemaakt van de robot.txt op de websites, aangezien hierin wordt aangegeven door de ontwikkelaars van de website wat wel of niet mag, scrapen daaronder vallende.[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import mechanize\n",
    "from googleapiclient.discovery import build\n",
    "import concurrent.futures\n",
    "from os import environ\n",
    "from urllib.error import HTTPError\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "\n",
    "Zoals eerder gezegt is het grootste deel van de scraper hergebruikt uit het vorige onderzoek, met als enige grootste verschillen dat het in een try-except is gezet vanwege het feit dat er een exception wordt aangeroepen als een webpagina niet gescrapet kan worden, en dat er alleen gescrapet wordt op de paragraph elementen op een webpagina, aangezien uit eigen onderzoek hier meestal de tekstdata wordt neergezet op webpagina's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scraper(webpage, br):\n",
    "    # Roep een try aan, omdat volgends robots.txt sommige websites niet gescrapet mogen worden, die een exception aanroepen wanneer ze gescrapet worden door de functie.\n",
    "    try:\n",
    "        # Haal de webdata op van de gegeven webpagina met de bijgeleverde browser.\n",
    "        data = br.open(webpage).get_data()\n",
    "        soup = BeautifulSoup(data)\n",
    "\n",
    "        # Haal de paragraph onderdelen op van de webpagina, aangezien meeste websites hierin textvelden hebben staan.\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Velden met tekst worden leeggehaald als de tekstvelden er zijn, en als laatste worden ze samengevoegt en uitgeprint\n",
    "        if paragraphs:\n",
    "            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return article_text\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return \"Blocked by robots.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine\n",
    "\n",
    "Voordat er gebruik gemaakt kan worden van de Search engine API van Google moet er een api_key en een cse_id aangemaakt worden op de cloud development panel. Voor ons project is er gekozen voor de gratis variant, die de mogelijkheid geeft om 100 queries per dag uit te voeren, wat vertaald naar ongeveer 1000 links per dag die opgehaald kunnen worden. Om ervoor te zorgen dat er genoeg data opgehaald wordt van de webpagina's worden er 5 queries uitgevoerd om links op te halen, wat in totaal een maximum van 50 links is die opgehaald worden. Hiervoor is gekozen vanwege het feit dat tijdens het onderzoeken van de queries het limiet vrij snel aangetikt werd, waardoor er een limiet werd gezet op 50 links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query):\n",
    "    # API key en Search engine keys voor het gebruik van de Google API\n",
    "    api_key = \"AIzaSyBI5TGhJH0WAvYIzuuNrttMbC4lxUBy0Mw\"\n",
    "    cse_id = \"5141af20165024a52\"\n",
    "\n",
    "    print(api_key)\n",
    "    print(cse_id)\n",
    "\n",
    "    # Roep de Search Engine aan, en haal indien mogelijk de eerste 50 results op voor de gegeven query\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    try:\n",
    "        res = service.cse().list(q=query, cx=cse_id).execute()\n",
    "        res2 = service.cse().list(q=query, cx=cse_id, start=11).execute()\n",
    "        res3 = service.cse().list(q=query, cx=cse_id, start=21).execute()\n",
    "        res4 = service.cse().list(q=query, cx=cse_id, start=31).execute()\n",
    "        res5 = service.cse().list(q=query, cx=cse_id, start=41).execute()\n",
    "    except HTTPError as e:\n",
    "        if e.resp.status == 429:\n",
    "            print(\"Quota exceeded. Please try again later.\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "    print(\"Links get!\")\n",
    "\n",
    "    # Plaats alle res elementen in een array\n",
    "    res_list = [res, res2, res3, res4, res5]\n",
    "\n",
    "    # Loop door de res_list en voeg de items samen als ze bestaan\n",
    "    items = []\n",
    "    for r in res_list:\n",
    "        if 'items' in r:\n",
    "            items.extend(r['items'])\n",
    "\n",
    "    return items if items else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler\n",
    "\n",
    "De Crawler gebruikt de hierboven gemaakte functies om met behulp van google 50 links op te halen en te scrapen. Hiervoor wordt er eerst een lijst met websites toegevoegd die niet gescrapet moeten worden, zoals social media websites, omdat dit in theorie blogs zijn waar iedereen informatie op kan zetten, al is het waar of niet. **Dit is gedaan aan de hand van een interview met een indirecte stakeholder, die aangaf dat hij dit zelf vaak ook doet, omdat er op meeste van deze sites geen nuttige inhoudelijke informatie bevatten rondt de bedrijven, en soms zelfs fake news zijn.** Daarnaast zit er een ThreadPoolExecutor in, waarmee er een timout exception aangeroepen kan worden als het langer duurt dan 10 seconden aangezien sommige webpagina's kunnen blijven hangen tijdens het webscrapen. Tot slot worden de opgehaalde texten gecheckt of het wel daadwerkelijk de texten zijn, waarna ze worden teruggegeven in een lijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spider(query):\n",
    "    # Creeër een browser die handhaaft op robots.txt, zodat alleen websites die gescrapet mogen worden gescrapet worden\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_robots(True)\n",
    "    # Voeg een lijst met exclude websites toe aan de query, zodat hier geen resultaten voor worden weergegeven\n",
    "    query_extra = ' -youtube.com -facebook.com -x.com -twitter.com -instagram.com -reddit.com -tiktok.com -threads.com -tripadvisor.com -pinterest.com'\n",
    "    query += query_extra\n",
    "    print(\"Getting links...\")\n",
    "\n",
    "    # Roep functie aan die google results ophaalt aan de hand van de gegeven query\n",
    "    results = google_search(query)\n",
    "    if not results:\n",
    "        return []\n",
    "    print(f\"Total results: {len(results)}\")\n",
    "\n",
    "    returns = []\n",
    "    count = 0\n",
    "\n",
    "    # Scrape alle links die opgehaald zijn uit de query\n",
    "    for result in results:\n",
    "        count +=1\n",
    "        title = result['title']\n",
    "        link = result['link']\n",
    "        # Voer de scraper uit met een threadpool, zodat een timeout aangeroepen kan worden als het scrapen van een link te lang duurt\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            try:\n",
    "                future = executor.submit(Scraper, result['link'], br)\n",
    "                text = future.result(timeout=10)\n",
    "            except TimeoutError:\n",
    "                text = \"Timeout\"\n",
    "\n",
    "        # Voeg de gescrapede text toe aan de geldige results list als deze niet leeg zijn of een exception aanriepen.\n",
    "        with open(\"results.csv\", \"a\", encoding='utf-8', newline='') as csvfile:\n",
    "            if text not in ['Blocked', 'Timeout', None, '', ' ', '  ', 'Blocked by robots.txt']:\n",
    "                csvfile.write(text + \"\\n\")\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting links...\n",
      "AIzaSyBI5TGhJH0WAvYIzuuNrttMbC4lxUBy0Mw\n",
      "5141af20165024a52\n",
      "Links get!\n",
      "Total results: 50\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # De naam van een persoon als test om de scraper te testen\n",
    "    Spider(\"Freddy Mercury\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "\n",
    "In conclusie is er met behulp van de gemaakte scraper en crawler een mogelijkheid gecreeërd om aan de hand van een gegeven query webpagina's op te halen, waar uiteindelijk een sentimentanalyse op uitgevoerd zou kunnen worden. Voor het testen van de scraper en de rest van de systemen zijn 20 queries per dag voldoende, maar als er uiteindelijk in de praktijk gebruik gemaakt gaat worden van dit product moet er gekeken worden naar de betaalde versie van de Google API, om zo indien nodig meer dan 20 queries uit te voeren.\n",
    "\n",
    "**Daarnaast moet er nog gekeken worden naar het aanroepen van een timeout binnen de functie, aangezien dit nog niet goed loopt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronnen\n",
    "\n",
    "[1] J. A. Chisty, “Innovation in co-creation practices: an exploratory study,” 2011, Accessed: Jan. 21, 2025. [Online]. Available: https://repository.library.carleton.ca/downloads/xs55mc780\n",
    "\n",
    "[2] F. McCown and M. L. Nelson, “Agreeing to disagree: Search engines and their public interfaces,” Proceedings of the ACM International Conference on Digital Libraries, pp. 309–318, 2007, doi: 10.1145/1255175.1255237.\n",
    "\n",
    "[3] P. Vyas, A. Chauhan, T. Mandge, and S. Hardikar, “Web crawler strategies for web pages under robot.txt restriction,” Aug. 2023, Accessed: Jan. 21, 2025. [Online]. Available: https://arxiv.org/abs/2308.04689v2\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
