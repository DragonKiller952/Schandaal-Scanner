{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper en -crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aan de hand van het vorige onderzoek, waarbij gekeken werd naar de beste methodes om een gegeven website te scrapen, werdt er aangegeven dat de vervolgstap zou zijn om het systeem te koppelen met een webcrawler om zo te kunnen zoeken naar websites aan de hand van een gegeven query. In dit document wordt er gekeken naar het maken van de crawler, waarbij gekeken wordt naar de beste methodes en hoe deze toegepast kunnen worden.\n",
    "\n",
    "## Scraper\n",
    "\n",
    "Zoals in het vorige onderzoek is aangegeven is er uiteindelijk gekozen voor een combinatie van Beautiful Soup en mechanize om data van webpagina's te scrapen. In dit onderzoek is dezelfde code hiervoor gebruikt, met een paar aanpassingen om het goed samen te laten werken met de rest van de systemen die ontwikkeld zijn.\n",
    "\n",
    "## Search engine\n",
    "\n",
    "Om te kunnen webcrawlen moet er een manier zijn om te kunnen zoeken naar websites om te scrapen. Een Search engine zoals google gebruiken is hiervoor een goede oplossing, aangezien dit de mogelijkheid geeft om aan de hand van een enkele zoek-query een lijst aan mogelijk nuttige links kan geven om te scrapen. Een probleem hiermee is echter dat er in de gebruiksvoorwaarden van google is aangegeven dat het niet is toegestaan om google te scrapen. Google heeft hier echter een oplossing op gevonden: de Google JSON Search API.\n",
    "\n",
    "### API\n",
    "\n",
    "De Google JSON Search API is een API gecreeërd door Google die gebruikers de mogelijkheid geeft om data op te halen uit de google search engine, zonder dat de daatwerkelijke search engine gescrapet hoeft te worden. Dit geeft een gemakkelijke en legale manier om links op te kunnen halen voor webscraping. Als gekeken wordt naar de top 10 resultaten wanneer zelf wordt gezocht tegenover de Google API, produceert het resultaten die 20% verschillen[1], maar aangezien het scrapen van data op search engines beperkt is en niet toegestaan is binnen hun gebruiksovereenkomst is het gebruiken van API's de optimale manier om data te verzamelen.[2]\n",
    "\n",
    "## Crawler\n",
    "\n",
    "De webcrawler, ook wel Spider genoemt, is een uiteindelijke combinatie van de search engine en de scraper. Hierbij haalt de crawler aan de hand van de gegeven query 50 links op, waarna hij deze een voor een scrapet en de uiteindelijke data returnt in een lijst. Bij het scrapen wordt er deze keer gebruik gemaakt van de robot.txt op de websites, aangezien hierin wordt aangegeven door de ontwikkelaars van de website wat wel of niet mag, scrapen daaronder vallende.[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import mechanize\n",
    "from googleapiclient.discovery import build\n",
    "import concurrent.futures\n",
    "from os import environ\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "\n",
    "Zoals eerder gezegt is het grootste deel van de scraper hergebruikt uit het vorige onderzoek, met als enige grootste verschillen dat het in een try-except is gezet vanwege het feit dat er een exception wordt aangeroepen als een webpagina niet gescrapet kan worden, en dat er alleen gescrapet wordt op de paragraph elementen op een webpagina, aangezien uit eigen onderzoek hier meestal de tekstdata wordt neergezet op webpagina's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scraper(webpage, br):\n",
    "    # Roep een try aan, omdat volgends robots.txt sommige websites niet gescrapet mogen worden, die een exception aanroepen wanneer ze gescrapet worden door de functie.\n",
    "    try:\n",
    "        # Haal de webdata op van de gegeven webpagina met de bijgeleverde browser.\n",
    "        data = br.open(webpage).get_data()\n",
    "        soup = BeautifulSoup(data)\n",
    "\n",
    "        # Haal de paragraph onderdelen op van de webpagina, aangezien meeste websites hierin textvelden hebben staan.\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Velden met tekst worden leeggehaald als de tekstvelden er zijn, en als laatste worden ze samengevoegt en uitgeprint\n",
    "        if paragraphs:\n",
    "            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return article_text\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return \"Blocked\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine\n",
    "\n",
    "Voordat er gebruik gemaakt kan worden van de Search engine API van Google moet er een api_key en een cse_id aangemaakt worden op de cloud development panel. Voor ons project is er gekozen voor de gratis variant, die de mogelijkheid geeft om 100 queries per dag uit te voeren, wat vertaald naar ongeveer 1000 links per dag die opgehaald kunnen worden. Om ervoor te zorgen dat er genoeg data opgehaald wordt van de webpagina's worden er 5 queries uitgevoerd om links op te halen, wat in totaal een maximum van 50 links is die opgehaald worden. Hiervoor is gekozen vanwege het feit dat tijdens het onderzoeken van de queries het limiet vrij snel aangetikt werd, waardoor er een limiet werd gezet op 50 links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query):\n",
    "    # API key en Search engine keys voor het gebruik van de Google API\n",
    "    api_key = environ.get('API_KEY')\n",
    "    cse_id = environ.get('CSE_ID')\n",
    "\n",
    "    # Roep de Search Engine aan, en haal indien mogelijk de eerste 50 results op voor de gegeven query\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=query, cx=cse_id).execute()\n",
    "    res2 = service.cse().list(q=query, cx=cse_id, start=11).execute()\n",
    "    res3 = service.cse().list(q=query, cx=cse_id, start=21).execute()\n",
    "    res4 = service.cse().list(q=query, cx=cse_id, start=31).execute()\n",
    "    res5 = service.cse().list(q=query, cx=cse_id, start=41).execute()\n",
    "    print(\"Links get!\")\n",
    "\n",
    "    # Kijk of de lijsten links bevatten, en voeg de gene die dat wel doen samen.\n",
    "    if 'items' in res5:\n",
    "        return res['items'] + res2['items'] + res3['items'] + res4['items'] + res5['items']\n",
    "    elif 'items' in res4:\n",
    "        return res['items'] + res2['items'] + res3['items'] + res4['items']\n",
    "    elif 'items' in res3:\n",
    "        return res['items'] + res2['items'] + res3['items']\n",
    "    elif 'items' in res2:\n",
    "        return res['items'] + res2['items']\n",
    "    elif 'items' in res:\n",
    "        return res['items']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler\n",
    "\n",
    "De Crawler gebruikt de hierboven gemaakte functies om met behulp van google 50 links op te halen en te scrapen. Hiervoor wordt er eerst een lijst met websites toegevoegd die niet gescrapet moeten worden, zoals social media websites, omdat dit in theorie blogs zijn waar iedereen informatie op kan zetten, al is het waar of niet. Daarnaast zit er een ThreadPoolExecutor in, waarmee er een timout exception aangeroepen kan worden als het langer duurt dan 10 seconden aangezien sommige webpagina's kunnen blijven hangen tijdens het webscrapen. Tot slot worden de opgehaalde texten gecheckt of het wel daadwerkelijk de texten zijn, waarna ze worden teruggegeven in een lijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spider(query):\n",
    "    # Creeër een browser die handhaaft op robots.txt, zodat alleen websites die gescrapet mogen worden gescrapet worden\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_robots(True)\n",
    "    # Voeg een lijst met exclude websites toe aan de query, zodat hier geen resultaten voor worden weergegeven\n",
    "    query_extra = ' -youtube.com -facebook.com -x.com -twitter.com -instagram.com -reddit.com -tiktok.com -threads.com -tripadvisor.com -pinterest.com'\n",
    "    query += query_extra\n",
    "    print(\"Getting links...\")\n",
    "\n",
    "    # Roep functie aan die google results ophaalt aan de hand van de gegeven query\n",
    "    results = google_search(query)\n",
    "    print(f\"Total results: {len(results)}\")\n",
    "\n",
    "    returns = []\n",
    "    count = 0\n",
    "\n",
    "    # Scrape alle links die opgehaald zijn uit de query\n",
    "    for result in results:\n",
    "        count +=1\n",
    "        title = result['title']\n",
    "        link = result['link']\n",
    "        print(f\"{count} {title} {link}\")\n",
    "        # Voer de scraper uit met een threadpool, zodat een timeout aangeroepen kan worden als het scrapen van een link te lang duurt\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            try:\n",
    "                future = executor.submit(Scraper, result['link'], br)\n",
    "                text = future.result(timeout=10)\n",
    "            except TimeoutError:\n",
    "                text = \"Timeout\"\n",
    "\n",
    "        # Voeg de gescrapede text toe aan de geldige results list als deze niet leeg zijn of een exception aanriepen.\n",
    "        if text not in ['Blocked', 'Timeout', None, '', ' ', '  ']:\n",
    "            returns.append(text)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting links...\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=%22Freddy+Mercury%22+-youtube.com+-facebook.com+-x.com+-twitter.com+-instagram.com+-reddit.com+-tiktok.com+-threads.com+-tripadvisor.com+-pinterest.com&cx=8085d3b6b82a2438b&start=11&key=AIzaSyAAaupkqmssdvamxKLnAIfQtwCGvgiZUgY&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:746458258038'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:746458258038'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# De naam van een persoon als test om de scraper te testen\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     test_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFreddy Mercury\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Spider(test_query):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mSpider\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting links...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Roep functie aan die google results ophaalt aan de hand van de gegeven query\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m google_search(query)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m returns \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mgoogle_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      7\u001b[0m service \u001b[38;5;241m=\u001b[39m build(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m      8\u001b[0m res \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mcse()\u001b[38;5;241m.\u001b[39mlist(q\u001b[38;5;241m=\u001b[39mquery, cx\u001b[38;5;241m=\u001b[39mcse_id)\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[0;32m----> 9\u001b[0m res2 \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mcse()\u001b[38;5;241m.\u001b[39mlist(q\u001b[38;5;241m=\u001b[39mquery, cx\u001b[38;5;241m=\u001b[39mcse_id, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     10\u001b[0m res3 \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mcse()\u001b[38;5;241m.\u001b[39mlist(q\u001b[38;5;241m=\u001b[39mquery, cx\u001b[38;5;241m=\u001b[39mcse_id, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     11\u001b[0m res4 \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mcse()\u001b[38;5;241m.\u001b[39mlist(q\u001b[38;5;241m=\u001b[39mquery, cx\u001b[38;5;241m=\u001b[39mcse_id, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m)\u001b[38;5;241m.\u001b[39mexecute()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=%22Freddy+Mercury%22+-youtube.com+-facebook.com+-x.com+-twitter.com+-instagram.com+-reddit.com+-tiktok.com+-threads.com+-tripadvisor.com+-pinterest.com&cx=8085d3b6b82a2438b&start=11&key=AIzaSyAAaupkqmssdvamxKLnAIfQtwCGvgiZUgY&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:746458258038'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:746458258038'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # De naam van een persoon als test om de scraper te testen\n",
    "    test_query = '\"Freddy Mercury\"'\n",
    "        \n",
    "    for i in Spider(test_query):\n",
    "        print(f\"[[{i}]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "\n",
    "In conclusie is er met behulp van de gemaakte scraper en crawler een mogelijkheid gecreeërd om aan de hand van een gegeven query webpagina's op te halen, waar uiteindelijk een sentimentanalyse op uitgevoerd zou kunnen worden. Voor het testen van de scraper en de rest van de systemen zijn 20 queries per dag voldoende, maar als er uiteindelijk in de praktijk gebruik gemaakt gaat worden van dit product moet er gekeken worden naar de betaalde versie van de Google API, om zo indien nodig meer dan 20 queries uit te voeren.\n",
    "\n",
    "**Daarnaast moet er nog gekeken worden naar het aanroepen van een timeout binnen de functie, aangezien dit nog niet goed loopt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronnen\n",
    "\n",
    "[1] J. A. Chisty, “Innovation in co-creation practices: an exploratory study,” 2011, Accessed: Jan. 21, 2025. [Online]. Available: https://repository.library.carleton.ca/downloads/xs55mc780\n",
    "\n",
    "[2] F. McCown and M. L. Nelson, “Agreeing to disagree: Search engines and their public interfaces,” Proceedings of the ACM International Conference on Digital Libraries, pp. 309–318, 2007, doi: 10.1145/1255175.1255237.\n",
    "\n",
    "[3] P. Vyas, A. Chauhan, T. Mandge, and S. Hardikar, “Web crawler strategies for web pages under robot.txt restriction,” Aug. 2023, Accessed: Jan. 21, 2025. [Online]. Available: https://arxiv.org/abs/2308.04689v2\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
